# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tVynUFJtMuz0IkznGrFLzkUM37_7XTNP
"""
import argparse
import os
# Commented out IPython magic to ensure Python compatibility.

from inspect import isfunction
from pathlib import Path

import matplotlib.pyplot as plt
import requests
from torch.optim import Adam
from torch.utils.data import DataLoader
from torchvision.utils import save_image

import torch.nn.functional as F

from diffuseNew.models.loss import p_losses, reconstruction_error_by_class
from diffuseNew.models.unet import *
from diffuseNew.utils.lib import num_to_groups
from diffuseNew.utils.sampling import sample
from diffuseNew.utils.schedule import *
from diffuseNew.utils.transforms import get_noisy_image, transform, transform_dataset
from diffuseNew.utils.visualization import plot
from datasets import load_dataset, Image, concatenate_datasets

torch.manual_seed(0)


import logging

logging.basicConfig(level=logging.INFO,  # Set the logging level
                    format='%(asctime)s - %(levelname)s - %(message)s',  # Log format
                    handlers=[
                        logging.FileHandler("training.log"),  # Log to a file
                        logging.StreamHandler()  # Also log to the console
                    ])

logger = logging.getLogger(__name__)

def create_imbalanced_dataset(dataset, class_samples):
    subsets = []
    for label, max_samples in class_samples.items():
        # Filter the dataset for the specific label
        filtered_data = dataset.filter(lambda example: example['label'] == label)
        # Select a subset if max_samples is specified and less than the available examples
        if max_samples is not None and len(filtered_data) > max_samples:
            filtered_data = filtered_data.shuffle(seed=42).select(range(max_samples))
        subsets.append(filtered_data)
    # Concatenate all subsets into one dataset using the correct concatenate method
    return concatenate_datasets(subsets)

def load_train_data(batch_size):
    train_dataset = load_dataset("mnist", split='train')
    class_samples = {0: 6000, 1: 6000, 2: 50, 3: 100, 4: 5500, 5: 5500, 6: 5500, 7: 5500, 8: 6000, 9: 6000}
    train_dataset = create_imbalanced_dataset(train_dataset, class_samples)
    train_dataset.with_transform(transform_dataset)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
    class_counts = {label: sum(1 for x in train_dataset['label'] if x == label) for label in range(10)}
    print("Class counts in the imbalanced dataset:")
    for label, count in class_counts.items():
        print(f"Class {label}: {count}")
    return train_loader

def load_test_data(batch_size):
    test_dataset = load_dataset("mnist", split='test')
    test_dataset.with_transform(transform_dataset)

    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    return test_loader

def train():

    # Setup parameters
    image_size = 28
    channels = 1
    batch_size = 128
    timesteps = 1000  # Number of diffusion timesteps
    epochs = 15  # Number of training epochs

    # Create a DataLoader for training data
    dataloader = load_train_data(batch_size)

    # Check a sample batch to ensure data format is correct
    batch = next(iter(dataloader))
    print(batch.keys())  # Should show 'pixel_values' and possibly 'labels'

    # Create a directory to save results and checkpoints
    results_folder = Path("./results")
    results_folder.mkdir(exist_ok=True)
    checkpoint_folder = Path("./checkpoints")
    checkpoint_folder.mkdir(exist_ok=True)
    save_and_sample_every = 1000

    # Set device to GPU if available
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Initialize the model
    model = Unet(
        dim=image_size,
        channels=channels,
        dim_mults=(1, 2, 4)
    )
    model.to(device)

    # Set up the optimizer
    optimizer = Adam(model.parameters(), lr=1e-3)

    # Training loop
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        epoch_loss = 0  # To accumulate epoch loss

        for step, batch in enumerate(dataloader):
            optimizer.zero_grad()  # Reset gradients

            # Get batch data and move to device
            batch_size = batch["pixel_values"].shape[0]
            batch = batch["pixel_values"].to(device)

            # Sample random timesteps
            t = torch.randint(0, timesteps, (batch_size,), device=device).long()

            # Calculate loss
            loss = p_losses(model, batch, t, loss_type="huber")

            # Print loss at every 100 steps
            if step % 100 == 0:
                print(f"Step {step}: Loss = {loss.item():.4f}")

            # Backpropagation
            loss.backward()
            optimizer.step()

            # Accumulate loss for the epoch
            epoch_loss += loss.item()

            # Save generated images periodically
            if step != 0 and step % save_and_sample_every == 0:
                milestone = step // save_and_sample_every
                batches = num_to_groups(4, batch_size)
                all_images_list = [sample(model, batch_size=n, channels=channels) for n in batches]
                all_images = torch.cat(all_images_list, dim=0)
                all_images = (all_images + 1) * 0.5  # Scale back to [0, 1]
                save_image(all_images, str(results_folder / f'sample-{epoch}-{milestone}.png'), nrow=6)

        # Calculate and print average epoch loss
        avg_epoch_loss = epoch_loss / len(dataloader)
        print(f"Average Loss for Epoch {epoch + 1}: {avg_epoch_loss:.4f}")

        # Save model checkpoint after each epoch
        checkpoint_path = checkpoint_folder / f"model_epoch_{epoch + 1}.pth"
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_epoch_loss,
        }, checkpoint_path)
        print(f"Checkpoint saved at {checkpoint_path}")

def load_model(checkpoint_path, image_size=28, channels=1):
    """
    Load a saved model checkpoint for evaluation.

    Args:
        checkpoint_path (str or Path): Path to the checkpoint file.
        image_size (int): Size of the image input (default is 28 for MNIST).
        channels (int): Number of input channels (1 for grayscale images).
        device (str): Device to load the model on ('cuda' or 'cpu').

    Returns:
        model (torch.nn.Module): The loaded model with state_dict applied.
    """
    # Define the model architecture
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = Unet(
        dim=image_size,
        channels=channels,
        dim_mults=(1, 2, 4)
    )
    model.to(device)

    # Load checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device)

    # Load the state dictionaries
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"Loaded model from checkpoint: {checkpoint_path}")

    return model

def eval_recon_loss(epoch, image_size, channels):
    checkpoint_folder = Path("./checkpoints")
    checkpoint_path = checkpoint_folder / f"model_epoch_{epoch + 1}.pth"
    test_dataloader = load_test_data(batch_size=128)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = load_model(checkpoint_path, image_size, channels)
    reconstruction_error_by_class(model, test_dataloader, device=device)


def plot_noisy_image(timestep, image):
    noisy_dir = f"./noisy_images"
    if not os.path.exists(noisy_dir):
        os.makedirs(noisy_dir)
        logger.info(f"Created checkpoint directory at {noisy_dir}")

    # url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
    # image = Image.open(requests.get(url, stream=True).raw)  # PIL image of shape HWC
    x_start = transform(image).unsqueeze(0)
    print(x_start.shape)
    t = torch.tensor([timestep])
    a = get_noisy_image(x_start, t)
    print(a)
    plot([a,], image, save_path=f'{noisy_dir}/noisy_image.png')


def plot_noisy_image_timesteps(image, step=15, timesteps=300):
    checkpoint_dir = f"./noisy_images"
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
        logger.info(f"Created checkpoint directory at {checkpoint_dir}")

    x_start = transform(image).unsqueeze(0)
    for timestep in range(0, timesteps + 1, step):
        plot([get_noisy_image(x_start, torch.tensor([timestep]))], x_start, save_path=f'{checkpoint_dir}/noisy_image_{timestep}.png')



def parse_arguments():
    parser = argparse.ArgumentParser(description='Training Configuration for Diffusion Model')
    parser.add_argument('--mode', type=str, default='train', help='Train or Eval')
    # parser.add_argument('--device', type=str, default='cuda', help='Device to use for training')
    # parser.add_argument('--epochs', type=int, default=20, help='Number of training epochs')
    # parser.add_argument('--batch_size', type=int, default=64, help='Training batch size')
    # parser.add_argument('--learning_rate', type=float, default=0.0004, help='Optimizer learning rate')
    # parser.add_argument('--ddpm_timesteps', type=int, default=1000, help='Number of DDPM timesteps')
    # parser.add_argument('--cosine_warmup_steps', type=int, default=500, help='Warmup steps for learning rate scheduler')
    # parser.add_argument('--cosine_total_training_steps', type=int, default=10000, help='Total training steps for learning rate scheduler')
    # parser.add_argument('--n', type=int, default=1, help='Number of images per class')
    args = parser.parse_args()
    return args


if __name__ == '__main__':
    args = parse_arguments()
    logger.info("Running with the following configuration:")
    logger.info(f"Mode: {args.mode}")
    for arg, value in vars(args).items():
        logger.info(f"{arg}: {value}")
    # config = {
    #     'device': args.device,
    #     'epochs': args.epochs,
    #     'batch_size': args.batch_size,
    #     'class_counts': {0: 5000, 1:10, 2: 4400, 3: 4000, 4: 4800, 5: 4400, 6: 4200, 7: 4000, 8: 4000, 9: 10},
    #     'ddpm_scheduler_timestep': args.ddpm_timesteps,
    #     'cosine_warmup_steps': args.cosine_warmup_steps,
    #     'cosine_warmup_training_steps': args.cosine_total_training_steps,
    #     'learning_rate': args.learning_rate,
    #     'n': args.n
    # }

    if args.mode == 'train':
        train()
    # elif args.mode == 'eval':
    #     eval_recon(config)
    # elif args.mode == 'eval_diff':
    #     eval_noise_denoise(config)
    elif args.mode == 'eval_classwise':
        image_size = 28
        channels = 1
        batch_size = 128
        timesteps = 1000  # Number of diffusion timesteps
        epochs = 15
        eval_recon_loss(image_size, channels, timesteps)
    # elif args.mode == 'one':
    #     eval_one(config)
    else:
        logger.error("Invalid mode")


