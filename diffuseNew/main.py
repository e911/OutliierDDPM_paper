# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tVynUFJtMuz0IkznGrFLzkUM37_7XTNP
"""
import os
# Commented out IPython magic to ensure Python compatibility.

from inspect import isfunction
from pathlib import Path

import matplotlib.pyplot as plt
import requests
from torch.optim import Adam
from torch.utils.data import DataLoader
from torchvision.utils import save_image

import torch.nn.functional as F

from diffuseNew.models.loss import p_losses, reconstruction_error_by_class
from diffuseNew.models.unet import *
from diffuseNew.utils.lib import num_to_groups
from diffuseNew.utils.sampling import sample
from diffuseNew.utils.schedule import *
from diffuseNew.utils.transforms import get_noisy_image, transform, transform_dataset
from diffuseNew.utils.visualization import plot
from datasets import load_dataset, Image

torch.manual_seed(0)


import logging

logging.basicConfig(level=logging.INFO,  # Set the logging level
                    format='%(asctime)s - %(levelname)s - %(message)s',  # Log format
                    handlers=[
                        logging.FileHandler("training.log"),  # Log to a file
                        logging.StreamHandler()  # Also log to the console
                    ])

logger = logging.getLogger(__name__)

def load_train_data(batch_size):
    train_dataset = load_dataset("mnist", split='train')
    train_dataset.with_transform(transform_dataset)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
    return train_loader

def load_test_data(batch_size):
    test_dataset = load_dataset("mnist", split='test')
    test_dataset.with_transform(transform_dataset)

    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    return test_loader

def train():

    # Setup parameters
    image_size = 28
    channels = 1
    batch_size = 128
    timesteps = 300  # Number of diffusion timesteps
    epochs = 10  # Number of training epochs

    # Create a DataLoader for training data
    dataloader = load_train_data(batch_size)

    # Check a sample batch to ensure data format is correct
    batch = next(iter(dataloader))
    print(batch.keys())  # Should show 'pixel_values' and possibly 'labels'

    # Create a directory to save results and checkpoints
    results_folder = Path("./results")
    results_folder.mkdir(exist_ok=True)
    checkpoint_folder = Path("./checkpoints")
    checkpoint_folder.mkdir(exist_ok=True)
    save_and_sample_every = 1000

    # Set device to GPU if available
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Initialize the model
    model = Unet(
        dim=image_size,
        channels=channels,
        dim_mults=(1, 2, 4)
    )
    model.to(device)

    # Set up the optimizer
    optimizer = Adam(model.parameters(), lr=1e-3)

    # Training loop
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        epoch_loss = 0  # To accumulate epoch loss

        for step, batch in enumerate(dataloader):
            optimizer.zero_grad()  # Reset gradients

            # Get batch data and move to device
            batch_size = batch["pixel_values"].shape[0]
            batch = batch["pixel_values"].to(device)

            # Sample random timesteps
            t = torch.randint(0, timesteps, (batch_size,), device=device).long()

            # Calculate loss
            loss = p_losses(model, batch, t, loss_type="huber")

            # Print loss at every 100 steps
            if step % 100 == 0:
                print(f"Step {step}: Loss = {loss.item():.4f}")

            # Backpropagation
            loss.backward()
            optimizer.step()

            # Accumulate loss for the epoch
            epoch_loss += loss.item()

            # Save generated images periodically
            if step != 0 and step % save_and_sample_every == 0:
                milestone = step // save_and_sample_every
                batches = num_to_groups(4, batch_size)
                all_images_list = [sample(model, batch_size=n, channels=channels) for n in batches]
                all_images = torch.cat(all_images_list, dim=0)
                all_images = (all_images + 1) * 0.5  # Scale back to [0, 1]
                save_image(all_images, str(results_folder / f'sample-{epoch}-{milestone}.png'), nrow=6)

        # Calculate and print average epoch loss
        avg_epoch_loss = epoch_loss / len(dataloader)
        print(f"Average Loss for Epoch {epoch + 1}: {avg_epoch_loss:.4f}")

        # Save model checkpoint after each epoch
        checkpoint_path = checkpoint_folder / f"model_epoch_{epoch + 1}.pth"
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_epoch_loss,
        }, checkpoint_path)
        print(f"Checkpoint saved at {checkpoint_path}")

def load_model(checkpoint_path, image_size=28, channels=1):
    """
    Load a saved model checkpoint for evaluation.

    Args:
        checkpoint_path (str or Path): Path to the checkpoint file.
        image_size (int): Size of the image input (default is 28 for MNIST).
        channels (int): Number of input channels (1 for grayscale images).
        device (str): Device to load the model on ('cuda' or 'cpu').

    Returns:
        model (torch.nn.Module): The loaded model with state_dict applied.
    """
    # Define the model architecture
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = Unet(
        dim=image_size,
        channels=channels,
        dim_mults=(1, 2, 4)
    )
    model.to(device)

    # Load checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device)

    # Load the state dictionaries
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"Loaded model from checkpoint: {checkpoint_path}")

    return model

def eval_recon_loss(epoch, image_size, channels):
    checkpoint_folder = Path("./checkpoints")
    checkpoint_path = checkpoint_folder / f"model_epoch_{epoch + 1}.pth"
    test_dataloader = load_test_data(batch_size=128)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = load_model(checkpoint_path, image_size, channels)
    reconstruction_error_by_class(model, test_dataloader, device=device)


def plot_noisy_image(timestep, image):
    noisy_dir = f"./noisy_images"
    if not os.path.exists(noisy_dir):
        os.makedirs(noisy_dir)
        logger.info(f"Created checkpoint directory at {noisy_dir}")

    # url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
    # image = Image.open(requests.get(url, stream=True).raw)  # PIL image of shape HWC
    x_start = transform(image).unsqueeze(0)
    print(x_start.shape)
    t = torch.tensor([timestep])
    a = get_noisy_image(x_start, t)
    print(a)
    plot([a,], image, save_path=f'{noisy_dir}/noisy_image.png')


def plot_noisy_image_timesteps(image, step=15, timesteps=300):
    checkpoint_dir = f"./noisy_images"
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
        logger.info(f"Created checkpoint directory at {checkpoint_dir}")

    x_start = transform(image).unsqueeze(0)
    for timestep in range(0, timesteps + 1, step):
        plot([get_noisy_image(x_start, torch.tensor([timestep]))], x_start, save_path=f'{checkpoint_dir}/noisy_image_{timestep}.png')




