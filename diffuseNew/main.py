# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tVynUFJtMuz0IkznGrFLzkUM37_7XTNP
"""

# Commented out IPython magic to ensure Python compatibility.

from inspect import isfunction

import matplotlib.pyplot as plt
from torch.optim import Adam
from torch.utils.data import DataLoader
from torchvision.utils import save_image

import torch.nn.functional as F

from diffuseNew.models.loss import p_losses
from diffuseNew.models.unet import *
from diffuseNew.utils.lib import num_to_groups
from diffuseNew.utils.sampling import sample
from diffuseNew.utils.schedule import *
from diffuseNew.utils.transforms import get_noisy_image, q_sample, transforms, transforms_dataset
from diffuseNew.utils.visualization import plot, image, x_start

timesteps = 300

print(x_start.shape)

t = torch.tensor([40])

a = get_noisy_image(x_start, t)
print(a)
plot([a,])
# torch.manual_seed(0)
#
# plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])
#
# from datasets import load_dataset
#
# # load dataset from the hub
# dataset = load_dataset("fashion_mnist")
# image_size = 28
# channels = 1
# batch_size = 128
#
#
# transformed_dataset = dataset.with_transform(transforms_dataset).remove_columns("label")
#
# # create dataloader
# dataloader = DataLoader(transformed_dataset["train"], batch_size=batch_size, shuffle=True)
#
# batch = next(iter(dataloader))
# print(batch.keys())
#
# from pathlib import Path
#
# results_folder = Path("./results")
# results_folder.mkdir(exist_ok = True)
# save_and_sample_every = 1000
#
#
# device = "cuda" if torch.cuda.is_available() else "cpu"
#
# model = Unet(
#     dim=image_size,
#     channels=channels,
#     dim_mults=(1, 2, 4,)
# )
# model.to(device)
#
# optimizer = Adam(model.parameters(), lr=1e-3)
#
# epochs = 1
#
# for epoch in range(epochs):
#     for step, batch in enumerate(dataloader):
#       optimizer.zero_grad()
#
#       batch_size = batch["pixel_values"].shape[0]
#       batch = batch["pixel_values"].to(device)
#
#       # Algorithm 1 line 3: sample t uniformally for every example in the batch
#       t = torch.randint(0, timesteps, (batch_size,), device=device).long()
#
#       loss = p_losses(model, batch, t, loss_type="huber")
#
#       if step % 100 == 0:
#         print("Loss:", loss.item())
#
#       loss.backward()
#       optimizer.step()
#
#       # save generated images
#       if step != 0 and step % save_and_sample_every == 0:
#         milestone = step // save_and_sample_every
#         batches = num_to_groups(4, batch_size)
#         all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))
#         all_images = torch.cat(all_images_list, dim=0)
#         all_images = (all_images + 1) * 0.5
#         save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)
#
# samples = sample(model, image_size=image_size, batch_size=64, channels=channels)
#
# random_index = 5
# plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap="gray")
#
# # Create a new figure with a horizontal grid layout
# fig, axes = plt.subplots(1, int(timesteps/10), figsize=(int(timesteps/10) * 2, 2))
#
# # Remove spaces between the subplots
# plt.subplots_adjust(wspace=0, hspace=0)
#
# # Loop through each timestep and plot the corresponding image
# for i in range(timesteps):
#     if i % 10 == 0:
#       step = int(i/10)
#       axes[step].imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap="gray")
#       axes[step].axis('off')  # Remove axis for a cleaner look
#
# # Display the entire row of images
# plt.tight_layout(pad=0)  # Ensure no padding around the images
plt.show()

